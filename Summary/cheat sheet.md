# 机器学习/深度学习八股文相关
## L1 L2 正则化
- 正则化 : 限制权值大小, 避免过拟合 
- l1 权值绝对值的和 
- l2 权值平方和的1/2 
    - L1 实质加入 <font color="red"> 拉普拉斯先验分布 </font>
    - L2 加入 <font color="red"> 零均值高斯先验分布 </font>
- L1 使得权值每次减少一个固定值, 最后权值可能为0, 产生**稀疏**效果(因而可以**特征选择, 降低维度**)
- L2 使得权值每次减少1/2, 只会趋近0, 产生**平滑**效果  
- 从二维空间的角度, L1正则项相当于为 <font color="red"> 参数定义了一个菱形解空间 </font>, L2正则项相当于为 <font color="red"> 参数定义了一个圆形解空间 </font>. 当  **原来 目标函数的最优解不是恰好落在解空间内**,  那么约束条件下的最优解 <font color="red"> 一定在解空间的边界上 </font>, 而 **L1 “棱角分明” 的解空间更容易与目标函数等高线在角点碰撞**, 从而产生稀疏解 

## 线性回归 
-  <font color="red"> 最小二乘回归(线性回归) </font> 的 **5个基本假设** 
    - X 与 Y标 满足线性&可加性 
    - X的各维度相互独立, 若不独立, 导致**多重共线性**
        - 两两之间使用 <font color="red"> 皮尔逊相关系数 </font> 
        - 多变量之间使用 <font color="red"> VIF 方差膨胀因子  </font> 检验是否存在多重共线性。若 **VIF超过10, 则说明存在严重的多重共线性**，导致**估计量的标准差偏大，置信区间变宽** 
    - 误差服从 **期望为0, 方差为定值** 的 **正态分布**,  
        - 这两个假设条件保证，在小样本条件下，顺利进行假设检验 
        - 若方差不为定值，则是 **异方差性，使得置信区间变宽**, OLS所得参数的t检验值被高估
    - 误差之间相互独立  
        - 若不是相互独立, 则是误差项之间存在  <font color="red"> 自相关性 </font>，导致**预测值标准差小于真实值， 进而导致置信区间变窄** 

## 参数估计方法  
- 最小二乘估计：
    - 假设是 **样本服从高斯分布**, 是 **样本服从高斯分布** 条件下的最大似然估计 
    - 目标函数是 **预测值与真实值误差的平方和**
- 最大似然估计
    - 
- 最大后验概率估计  
    - 

- 贝叶斯估计
    - 

## LR  
- 逻辑回归, 叫做回归, 但却是用于解决二分类问题, 本质上还是线性回归
- 先进行特征的线性组合, 再使用**sigmoid**函数将结果映射到[0, 1]之间
- 参数估计方法: <font color="red"> 最大似然估计 </font>, 使用  <font color="red"> 梯度下降 或 拟牛顿法 </font> 进行学习
- 损失函数: <font color="red"> 交叉熵 </font> 
- 解决非线性问题: **特征变换** 和 **核函数** 
- 优缺点
    - 优点: 
        - 计算速度快, <font color="red"> 计算量仅和特征数量有关 </font>, 分布式实现较为容易, 可通过堆叠机器提高训练速度 
        - 方法简单, 可解释性强, 可以直接看到各个特征的权重 
        - 入模新变量方便 
    - 缺点
        - 数据预处理, 特征工程比较复杂, 需要归一化
        - 模型的能力有限 
        - 处理非线性数据较为复杂, <font color="red"> 只能直接处理线性可分数据 </font>. 要处理非线性数据, 引入 <font color="red"> 核函数 或 特征变换 </font>, 显式地将特征映射到高维 
        - 难以处理不平衡数据集 (正负样本比例及其悬殊)   

- 为什么要进行特征离散化处理
    - 离散特征方便增减, 便于模型迭代 
    - 离散化的数据对异常值具有鲁棒性, 大大降低异常值带来的干扰, 例如“年龄300” 
    - 离散化之后, 每个变量有单独的权重, 相当于引入了 <font color="red"> 非线性 </font>, 能提升模型的表达能力  
    - 离散化之后, 可进行 <font color="red"> 特征交叉, 从 M + N 个变成 M*N个 </font>, 进一步引入非线性 
    - 特征离散化之后, 模型会更稳定. 例如: 一个用户不会因为年龄加一就变成完全不同的人. 因此, **区间划分的边界** 也很重要   
- <font color="red"> 线性回归与逻辑回归的区别与联系 </font> 
    - 线性回归 假设误差服从  <font color="red"> 高斯分布 </font>, LR假设样本服从 <font color="red"> 伯努利分布 </font>    
    - 线性回归使用 <font color="red"> 最小二乘法 </font> 估计参数, LR使用 <font color="red"> 最大似然估计 </font> 来估计参数  

- <font color="red"> LR为什么不能使用最小二乘法 </font>, 而是使用最大似然估计  
    - 从 **假设条件** 的角度, 最小二乘法的假设前提是 <font color="red"> 误差满足高斯分布 </font>, 逻辑回归的假设是 <font color="red"> 样本满足二项分布 </font>
    - 从 **目标函数优化** 的角度, 最小二乘法优化的目标函数是 <font color="red"> 平方和 </font>， **非凸不易求解**, 容易得到 **局部最优解**。极大似然估计优化的目标函数是 **对数似然函数**, 是 **高阶连续可导的凸函数**，方便用凸优化算法求解    

- LR为什么使用 <font color="red">**对数损失函数(交叉熵损失函数)** </font>，而不是平方损失函数 ？ 
    - 概念不同 
        - 平方损失函数(MSE)：n个样本输出与期望输出的**差的平方的平均值** 
        - 交叉熵损失函数(CE) : 模型预测值与真实值之间的差异大小，越大代表越不相近 
    - 参数更新速度不同 
        - MSE受 sigmoid 函数影响, 参数更新与**样本**和**sigmoid函数的梯度**都有关，导数更新缓慢 
        - CE损失函数更新**只与误差有关**, **误差大时，权重更新快，误差小时，权重更新慢**
        - 当**sigmoid作为激活函数时**，若采用MSE函数, 这就是一个**非凸问题**，采用交叉熵损失函数，就是**凸优化问题** 
    - 使用场景不同 
        - MSE适用于回归
        - 交叉熵适用于分类 


## 决策树剪枝 
- 决策树生成算法本质是贪心算法, 节点划分过程中不断重复, 可能导致分支过多, 以至于噪音被当作数据的一般性质, 进而导致过拟合  
- **剪枝**是一种有效防止过拟合的方法, 将不必要的节点删去, 降低决策树的大小  
### 预剪枝 
- 在生成节点过程中, <font color="red"> 对每个节点在划分前进行评估 </font>, 评估方法为 **交叉验证法**. 如果 <font color="red"> 当前节点的划分在验证集中表现更差, 则提前停止划分 </font> , 并将当前节点标记为叶节点, 通过限制生长的方式避免过拟合 
### 后剪枝 
- 先利用数据生成一棵完整的树, 自底向上地对非叶子节点进行评估, <font color="red"> 如果该节点对应的子树替换为叶节点后, 验证集上误差更小, 则将该子树替换为叶节点 </font>  

## Adaboost  
- Adaptive boosting 是一种基于 Boosting 思想的算法, 将"弱学习算法" 提升为 “强学习算法” 
- 本身通过 **改变数据分布来实现**, 根据 <font color="red"> 每次训练集中每个样本的分类是否正确 </font>, 以及 <font color="red"> 上次的总体分类准确率 </font>, 来确定样本的权值. 将修改过权值的新数据送给下层分类器做训练 

### Adaboost 解决的2个关键问题 
- 每一轮如何改变样本的权值或概率分布
    - <font color="red"> 上一轮预测正确的样本权重降低, 上一轮预测错误的样本权重升高 </font>  
- 如何将弱分类器组合成一个强分类器
    - 弱分类器的组合, **加权多数表决** 
    - 增大 <font color="red"> 分类误差率较小的弱分类器的权值 </font>, 使其在表决中起较大作用. 
    - <font color="red"> 减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用 </font> 
    - 
### Adaboost使用m个基学习器和Bagging加权平均使用m个学习器之间有什么不同 
- Adaboost
    - Adaboost的基学习器是顺序关系, **第k个基学习器根据前k-1个学习器得到的误差更新数据分布**
    - 每一次的数据分布都不同，是使用 <font color="red"> 同一个学习器在不同的数据分布上进行学习 </font>  
- Bagging 
    - 加权平均的m个学习器是可以并行处理的，<font color="red"> 在同一个数据分布上，学习得到m个不同的学习器进行加权 </font>    
     
## RF  
- 随机性体现在哪里
    - 样本随机性 
    - 特征随机性 
    - 
- 
- 


## GBDT 


## XGB    
- 原理 
    - obj = 训练损失 + 正则化项
    - 正则项: 每棵树的节点个数 + 每个叶子节点分数的L2模平方 
    - 
- 泰勒展开, 为什么用二阶
    - 精准性 : 用二阶泰勒展开去拟合残差, 精度更高 
    - 可扩展性 : 用二阶展开, , 

- xgb对0和缺失值处理相同(视为稀疏矩阵), 考虑分配到左右子树的情况，根据增益大小决定分配到哪个子树 

- 为什么快
    - **分块并行**：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点；
    - **候选分位点**：每个特征采用常数个分位点作为候选分割点；
    - **CPU cache 命中优化**： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中；
    - **Block 处理优化**：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐   

- 评价特征的重要性
    - gain 
    - weight 
    - cover 

- xgb调参步骤 
    - 通用参数
        - booster
            - eta
            - min_child_weight：最小叶子节点样本权重和
            - max_depth：树的最大深度
            - max_leaf_nodes: 树的最大的叶子数量
            - gamma 
            - max_delta_step:每棵树权重改变的最大步长
        - silent 
        - nthread 
    - 通常使用网格搜索或贝叶斯优化
        - 固定学习率**learning rate**=0.1，初始化其他参数
            - max_depth = 5 
            - min_child_weight = 1 
            - gamma = 0 
            - subsample.cosample_butree = 0.8 
        - max_depth 和 min_child_weight 参数调优, 大范围粗调 + 小范围精调
        - gamma 
        - 调整 subsample 和 colsample_bytree 
        - 正则化参数调优，选择 L1 或者 L2 正则化 
        - 降低学习率，得到最佳学习率数值 
    - n_estimator (树的个数) 
    - subsample: 样本子集的样本量
    - eta 一般[0.1, 0.2] 
    - Gamma: 用来防止过拟合，树的节点上进行进一步分支所需要的最小目标函数减少量 
    - 用于**剪枝**的参数
        - max_depth
        - colsample_bytree 
        - colsample_bylevel 
        - colsample_bynode 
- 调参顺序 
    - 集成模型
        - n_estimators 
        - learning_rate 
        - silent 
        - subsample 
    - 弱分类器  
        - max_depth 
        - objective  
        - booster 
        - gamma 
        - colsample_bytree 
        - colsample_bylevel 
        - reg_alpha 
        - reg_lambda   
    - 其他过程  
        - scale_pos_weight 
        - nthreads 
    - 顺序
        - n_estimator & eta
        - max_depth OR Gamma 
        - subsample 和 剪枝参数 (colsample_bytree, colsample_bylevel) 
        - alpha, lambda 

# 深度学习 
## 梯度消失
- 现象
    - 靠近输出的层权值参数正常变化, 靠近输入的层权值参数几乎不变 
- 原因
    - 从网路的角度: 层数过深, 梯度的变化在反向传递时, 经过多次累乘,趋近于0 
    - 从激活函数的角度: 当输入值落入激活函数的饱和区, 导致导数为一个比较小的值, 多个值累乘导致反向传播的梯度趋近于零
- 解决方法
    - 按值截断, 把梯度压缩到一个范围内
    - 按模截断, 若大于阈值, 则 $threshold * \textbf{g}/ |g| $

## 梯度爆炸 
- 现象
    - 靠近输出的层参数变动剧烈, 靠近输入的层参数几乎不变 
    - 权重很快变成 NaN 
    - 损失函数变为 NaN  
- 原因
    - 权值初始化不合适, 初始权值使得损失函数发生剧烈波动 
- 解决方法
    - 梯度截断 
## 激活函数 
- Sigmoid  
- Tanh 
- ReLU 
- Leaky ReLU 


## DL解决过拟合 
- 从数据的角度
    - 添加噪声
    - 数据增强 
- 从模型的角度
    - early stopping : 在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练
    - L1 / L2 正则化 
    - Dropout : 
        - 实现: 让 <font color="red"> 神经元的输出以一定概率变为0 </font>  
        - 原理: 
            - 相当于对多个神经网络取平均 
            - 减少神经元之间的共适应关系 
    - BatchNormalization  
        - 解决 **Interval covariate shift** 问题 

## 优化算法 
- MBGD: 每次利用一个 mini-batch 数据更新参数 
- Momentum: 参考动量的概念, 将前几次的梯度也加入到当前计算中, **梯度不变的方向上速度变快**, <font color="red"> 加速收敛 </font>  
- adagrad: 在训练过程中自动变更学习速率, 
- adam: 利用梯度的 一阶矩估计 和 二阶矩估计, 动态的调整每个参数的learning rate, 为不同的参数设置独立的**自适应学习率** 



## CNN 与 MLP 
- 引入卷积核
- 局部连接, 减少参数数量, 一定程度上减少过拟合  
- 权值共享, 减少参数的数量
- 平移不变性, 卷积+最大池化提供平移不变性, 即使平移, 卷积依然可以检测到特征


## RNN 与 LSTM 的对比 
- 细胞状态 和 门控机制 
- 遗忘门控制上一时刻的遗忘程度 

## Transformer 的结构和组件
- Encoder-Decoder 结构 
- Encoder 
    - 6个相同的大模块堆叠
        - 2个多头self-attension子模块 
        - 1个前馈神经网络模块  
    - 最底下的模块输入时 embedding (如 word2vec 的输出)


- Decoder 
    - 6个大模块 
        - 多头 self-attension 
        - 多头 Encoder-Decoder 交互模块
        - 前馈神经网络模块 

- 运行过程 
    - 输入序列通过 word2vec 转为词向量
    - 词向量经过 positional encoding  
    - 经过位置编码的词向量通过3个权值矩阵, 计算得 Q, K, V向量
    - Q K V向量用于计算 <font color="red"> attention 值 </font> 
        - 计算 <font color="red"> 每个单词之间的相关性得分 </font>, <font color="red"> Q中每个向量和K中每个向量做点积 </font> 
        - 对相关性得分做归一化, 目的在于<font color="red"> 训练时梯度能够稳定 </font> 
        - 

## 为什么要添加位置编码 
- 因为句子中词语出现的位置不同, 含义也不同, 而 self-attention无法获取位置信息, 需要位置编码信息来确定一个单词的位置 


## Transformer 的优势, 相比于RNN 解决了什么问题  


## Transformer 相比于 CNN, 优势在哪里 

## 评价指标  
- 分类指标
    - 混淆矩阵
        - precision: TP / TP + FP
        - recall: TP / TP + FN   

    - F1 score  
        - 由于精确率和召回率不可能都高, 如何平衡二者, 让二者的差异不能过大
        - 使用 F1 = 2 * (P * R) / ( P + R), 即 上乘下加, **加权调和平均数**的倒数 
   
    - PR 
        - 横坐标 Recall 
        - 纵坐标 Precision 
        - 曲线拐点约靠近右上角, 性能越好 
        - **PR曲线在数据不平衡条件下, 不稳定, 故采用ROC** 
        - 本质是, 不同阈值下, 找出每一对(查准率, 查全率) 
        - 
     - ROC 
        - 纵轴TPR, 横轴 FPR 
        - 

    - AUC  
        - ROC曲线下的面积 
        - TPR (recall = TP / (TP + FN)) 
        - FPR (FP / FP + TN)   
        - 如果ROC面积越大，说明曲线越往左上角靠过去。那么对于任意截断点，(FPR，TPR)坐标点越往左上角（0,1）靠，说明FPR较小趋于0（根据定义得知，就是在所有真实负样本中，基本没有预测为正的样本），TRP较大趋于1（根据定义得知，也就是在所有真实正样本中，基本全都是预测为正的样本）。并且上述是对于任意截断点来说的，很明显，那就是分类器对正样本的打分基本要大于负样本的打分（一般预测值也叫打分），衡量的就是**排序能力** 
        - 由于AUC衡量的是一种排序能力
            - 设有 M 个正样本, N 个负样本. 对预测概率从高到低排序
            - 每个概率值设一个rank 
            - 对所有样本的预测值排序, 编号为 rk_i, 该排序代表了该概率超过的样本数量
            - 对于概率最大的正样本, 比它小的负样本个数为 **rk_1 - M**  
            - 对于概率次大的正样本, 比它小的负样本个数为 **rk_2 - (M-1)**, 以此类推
            - $ (\sum( rk_{i} - M(M+1)/2)) / (M * N) $  

        - 优缺点
            - 
    
```python 
def calAUC(prob, labels):
    f = list(zip(prob, labels))
    rank = [values2 for values1, values2 in sorted(f, key=lambda x: x[0])]
    rankList = [i + 1 for i in range(len(rank)) if rank[i] == 1]
    posNum = 0
    negNum = 0
    for i in range(len(labels)):
        if (labels[i] == 1):
            posNum += 1
        else:
            negNum += 1
    auc = 0
    auc = (sum(rankList) - (posNum * (posNum + 1)) / 2) / (posNum * negNum)
    print(auc)
    return auc 

if __name__ == "__main__":
    labels = np.array([1, 0, 0, 0, 1, 0, 1, 0, ])
    prob = np.array([0.9, 0.8, 0.3, 0.1, 0.4, 0.9, 0.66, 0.7])

    fpr, tpr, thresholds = roc_curve(labels, prob, pos_label=1)
    print("-----sklearn:", auc(fpr, tpr))
    print("-----py脚本:", calAUC(prob, labels))
```  

- 回归指标 
    - MAE
        - 预测值与真实值之间的残差的绝对值的均值
        - L1范数误差
    - MSE 
        - 预测值与真实值之间的残差的平方和的均值
        - 如果存在离群点, 计算结果偏大, 对离群点较为敏感  
        - L2范数误差
    
    - RMSE 
        - 预测值与真实值的残差的标准差 
    - R2
        - 当前模型与基准模型的对比
        - 基准模型: y标均值与y标的误差平方和
        - 1 - (MSE(当前模型)) / (MSE(基准模型)) 
    - 
 
## 风控相关 
### 违约预测模型开发
    - 图谱需求
    - 用户画像需求
        - 静态标签
        - 偏好标签
        - 活动活跃度指标
        - 资产指标  
    - 违约预测模型
        - 

### 反洗钱模型开发
    - 相同收付款人频繁交易模型
    - 快进快出模型 
    - 涉恐模型

### 评分卡模型 
#### LR评分卡
- 变量选择
    - 表结构梳理
    - 变量分析与筛选 
        - 查看变量分布
        - 单变量分析
            - IV 
            - 稳定性
            - 覆盖率
        - 多变量分析 
            - 两两相关性分析 
            - 多重共线性 (方差膨胀因子, VIF, 用于筛选) 
        - 
    - 变量分箱
    - WOE编码 
    - WOE编码的好处
        - **使badrate呈非线性的变量转化为线性**, 使模型能有效应对异常值 
    - 变量选择
    - 
#### 集成模型评分卡 (XGB, LGB) 


- 技术指标  
    - WOE 
        - 两种理解: 
            - 每个分箱中 **坏样本分布** 与 **好样本分布** 之间的差异 (每个bin中 坏/好)
            - **每个分箱的坏好比** 与 **总体坏好比** 的差异 (每个bin / 整体) 
            - 
    - IV 
        - 衡量好样本与坏样本分布的差异 
        - IV值越大, 说明区分能力越大, 预测能力越强 

    - KS 
        - 横轴阈值, 纵轴TPR/FPR 
        - 
    - PSI 
        -  
    - CSI 
        - 
    - Lift 
        -   

- Viantage, 滚动率, 迁移率 
    - Viantage 账龄分析: 分析账户成熟期和变化规律 
    - 滚动率分析: 用以定义账户好坏程度 
    - 迁移率分析: 分析不同状态之间的转化率 
- MOB 账龄
    - 定义: 资产放款月份
    - MOB0: 放款日至当月底 
    - MOB1: 放款后第二个完整月
    - MOB2: 放款后第三个完整月  
    - MOB最大值取决于**产品周期** 
- DPD 逾期天数
    - 定义: 实际还款日 - 应还款日  
- M 逾期期数
    - 逾期天数 / 设定的区间间隔 
    - M0 : 逾期期数0, 也可用C表示 
    - M1 : 逾期 1-30 天 
    - M2 : 逾期 31-60 天  
 ## 噪声处理 
    - 数据清洗  
        - 缺失值
            - >30%以上尽量不使用 
            - <10%可用中位数和RF填充 
            - 10-30%可单独看作一个类或者分箱  
        - 查看各变量与目标变量之间的相关性
    - 异常值检测 
        - IF, LOF 
        - 
    - 

## 常见数据预处理方式 
- 聚集
- 抽样 
- 降维
- 特征子集选择 
- 特征创建
- 离散化和二元化
- 变量变换 

- 变量筛选 
    - 唯一变量直接剔除
    - 方差很小小于阈值的变量直接剔除  
- 缺失值 
    - 处理方法 
        - 直接使用
        - 直接删除
        - 补全 
            - 均值插补 
            - 同类均值插补 
            - 建模预测 
            - 高维映射 
            - 多重插补 
            - 压缩感知及矩阵补全  
- 异常值 
    - 
## 特征筛选
- 过滤法
    - 离散型:  
    - Filter(过滤法)：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选 
    - Person相关系数, 互信息和最大信息系数
- 封装法  
    - Wrapper(包装法)：对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集 
    - 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后通过学习器返回的 coef_ 或者feature_importances_ 消除若干权重较低的特征, sklearn的RFE 
    - 
- 嵌入法 


- 降维
    - PCA 
        - 原理: 通过找到低维空间中数据方差最大的方向, 使得投影后的数据最大程度上保留原始信息  
        - 均值和协方差, **数据中心化处理**保证特征均值为0 
        - 求协方差矩阵, 衡量特征之间的相关性
        - 计算特征值和特征向量
        - 选取数值较大的特征值所对应的特征向量, 原数据与这些向量相成, 得到的就是投影
## 图算法  
- leidian: 目前最新的社区划分算法, 基于
- louvain: 基于模块度 
- LPA 
- Infomap 

# 异常检测  
- 孤立森林 LF 
- LOF 局部因子检测 
- 



# 客户画像 
## 客户标签  
- 静态标签
    - 以家庭为单元:
        - 配偶特征: 国籍, 工作单位类型, **月薪收入** 等
        - 家庭特征: 总人数, 供养人口, 可支配年收入, **房产性质**,  **房产套数**, **住宅面积**, **是否有车**, **月收入**,  **月平均支出**, **月债务支出**, **总负债**, **贷款总笔数**
- 偏好标签
    - 各渠道上周交易量 
    - 小额取现上周交易量 
    - 持有产品上周余额总量 
    - 各产品上周持有金额总量 
    - 计算衰减 
## 客户活动指标活跃度统计 
- 对手活跃度 
- 卡活跃度 
- 客户活跃度 
- 渠道活跃度 
- 取现活跃度 
- 资金出入活跃度 
- POS交易商户活跃度  

## 客户收支情况统计  
- 存款收支统计 
- 贷款收支统计 
- 第三方京东收支统计  

## 客户资产指标统计 
- 产品持有明细
- 当前持有余额
- 资产汇总信息  

## 拉链表 
### 应用场景 
- 表中部分字段会被 update, 同时存在部分新增 
- 需要查看某一个时间点或者时间段的历史快照信息 
    - 查看某一产品在历史某一时点的历史状态 
    - 查看某一个用户在过去某一段时间内，更新过几次  
- 记录中 <font color="red"> 变化的比例和频率 </font> 不是很大 

### 实际方法
- 新增 **ETL_START** 和 **ETL_END** 两列, 为数据行的生命周期 
- <font color="red"> 不保存冗余数据 </font>, 只有某行 <font color="red"> 行的数据发生变化，才需要保存下来 </font> 
- **修改**: 修改后的记录, 生效日期更新, 失效日期为无穷大. 修改前的记录,  <font color="red"> 修改失效日期 </font> 
- 将新增数据和变化数据合并在一起 (Union All), 形成一个新的临时拉链表数据, 用临时拉链表覆盖旧的拉链表  






