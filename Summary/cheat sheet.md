# 机器学习八股文相关
## l1 l2 正则化
- 正则化 : 限制权值大小, 避免过拟合 
- l1 绝对值的和 
- l2 平方和的1/2 
- L1 实质加入 **拉普拉斯先验分布**, L2 加入**高斯先验分布** 
- L1 使得权值每次减少一个固定值, 最后权值可能为0, 产生**稀疏**效果(因而可以**特征选择, 降低维度**)
- L2 使得权值每次减少1/2, 只会趋近0, 产生**平滑**效果  
- 从二维空间的角度, L1正则项相当于为 <font color="red"> 参数定义了一个菱形解空间 </font>, L1正则项相当于为 <font color="red"> 参数定义了一个圆形解空间 </font>. 当  **原来 目标函数的最优解不是恰好落在解空间内**,  那么约束条件下的最优解 <font color="red"> 一定在解空间的边界上 </font>, 而 **L1 “棱角分明” 的解空间更容易与目标函数等高线在角点碰撞**, 从而产生稀疏解 

## lr 
- 
- 
- 
-  

## Adaboost 


## RF 


## GBDT 


## XGB    
- 原理 
    - obj: (结构部分 + 叶子节点权重) + 正则化 + 常数项  
    - 正则项: 每棵树的节点个数 + 每个叶子节点分数的L2模平方 
    - 
- 泰勒展开 
    - 

- xgb对0和缺失值处理相同(视为稀疏矩阵), 考虑分配到左右子树的情况，根据增益大小决定分配到哪个子树 

- 为什么快
    - **分块并行**：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点；
    - **候选分位点**：每个特征采用常数个分位点作为候选分割点；
    - **CPU cache 命中优化**： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中；
    - **Block 处理优化**：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐 


# 激活函数 
- Sigmoid  
- Tanh 
- ReLU 
- Leaky ReLU 

## 优化算法 
- MBGD: 每次利用一个 mini-batch 数据更新参数 
- Momentum: 参考动量的概念, 将前几次的梯度也加入到当前计算中, 
- adagrad: 在训练过程中自动变更学习速率, 
- adam: 利用梯度的 一阶矩估计 和 二阶矩估计


## CNN 与 MLP 
-  


## RNN 与 LSTM 的对比 
-  


## 评价指标  
- 混淆矩阵
    - precision: TP/TP+FP
    - recall: TP / TP + FN
- AUC  
    - ROC曲线下的面积 
    - TPR(recall) 
    - FRP(FP/FP+TN) 
- KS 
    - 横轴阈值, 纵轴TPR/FPR 
    - 
- PSI  

## 噪声处理 
    - 数据清洗  
        - 缺失值
            - >30%以上尽量不使用 
            - <10%可用中位数和RF填充 
            - 10-30%可单独看作一个类或者分箱  
        - 查看各变量与目标变量之间的相关性
    - 异常值检测 
        - IF, LOF 
        - 
    -  

## 特征筛选
- 过滤法
    - 离散型: 
- 封装 
- 嵌入法 






# 算法题相关
## 双指针 
- 快慢: 是否有环 
- 

## 滑动窗 
- 无重复字符最长


## 状态机 DP 
- 

## 树形DP 
- 打家劫舍I II III  

## 二叉树 
- 基于颜色标记法(辅助栈)的迭代 
- 递归   


# 风控项目相关
## 图算法 
- louvain: 基于模块度



