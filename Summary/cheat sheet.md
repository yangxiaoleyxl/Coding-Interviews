# 机器学习/深度学习八股文相关
## l1 l2 正则化
- 正则化 : 限制权值大小, 避免过拟合 
- l1 绝对值的和 
- l2 平方和的1/2 
- L1 实质加入 **拉普拉斯先验分布**, L2 加入**零均值高斯先验分布** 
- L1 使得权值每次减少一个固定值, 最后权值可能为0, 产生**稀疏**效果(因而可以**特征选择, 降低维度**)
- L2 使得权值每次减少1/2, 只会趋近0, 产生**平滑**效果  
- 从二维空间的角度, L1正则项相当于为 <font color="red"> 参数定义了一个菱形解空间 </font>, L2正则项相当于为 <font color="red"> 参数定义了一个圆形解空间 </font>. 当  **原来 目标函数的最优解不是恰好落在解空间内**,  那么约束条件下的最优解 <font color="red"> 一定在解空间的边界上 </font>, 而 **L1 “棱角分明” 的解空间更容易与目标函数等高线在角点碰撞**, 从而产生稀疏解 

## lr 
- 逻辑回归, 叫做回归, 但却是用于解决二分类问题, 本质上还是线性回归
- 先进行特征的线性组合, 再使用**sigmoid**函数将结果映射到[0, 1]之间
- 参数估计方法: <font color="red"> 最大似然估计 </font>, 使用  <font color="red"> 梯度下降 或 拟牛顿法 </font> 进行学习
- 损失函数: <font color="red"> 交叉熵 </font> 
- 解决非线性问题: **特征变换** 和 **核函数** 
- 优缺点
    - 优点: 
        - 计算速度快, <font color="red"> 计算量仅和特征数量有关 </font>, 分布式实现较为容易, 可通过堆叠机器提高训练速度 
        - 方法简单, 可解释性强, 可以直接看到各个特征的权重 
        - 入模新变量方便 
    - 缺点
        - 数据预处理, 特征工程比较复杂, 需要归一化
        - 模型的能力有限 
        - 处理非线性数据较为复杂, <font color="red"> 只能直接处理线性可分数据 </font>. 要处理非线性数据, 引入 <font color="red"> 核函数 或 特征变换 </font>, 显式地将特征映射到高维 
        - 难以处理不平衡数据集 (正负样本比例及其悬殊)  
- 为什么要进行特征离散化处理
    - 离散特征方便增减, 便于模型迭代 
    - 离散化的数据对异常值具有鲁棒性, 大大降低异常值带来的干扰, 例如“年龄300” 
    - 离散化之后, 每个变量有单独的权重, 相当于引入了 <font color="red"> 非线性 </font>, 能提升模型的表达能力  
    - 离散化之后, 可进行 <font color="red"> 特征交叉, 从 M + N 个变成 M*N个 </font>, 进一步引入非线性 
    - 特征离散化之后, 模型会更稳定. 例如: 一个用户不会因为年龄加一就变成完全不同的人. 因此, **区间划分的边界** 也很重要  
-  

## Adaboost 


## RF 


## GBDT 


## XGB    
- 原理 
    - obj = 训练损失 + 正则化项
    - 正则项: 每棵树的节点个数 + 每个叶子节点分数的L2模平方 
    - 
- 泰勒展开, 为什么用二阶
    - 用二阶泰勒展开去拟合残差
    - 用二阶展开, 

- xgb对0和缺失值处理相同(视为稀疏矩阵), 考虑分配到左右子树的情况，根据增益大小决定分配到哪个子树 

- 为什么快
    - **分块并行**：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点；
    - **候选分位点**：每个特征采用常数个分位点作为候选分割点；
    - **CPU cache 命中优化**： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中；
    - **Block 处理优化**：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐   

- 评价特征的重要性
    - gain 
    - weight 
    - cover 

- xgb调参步骤 
    - 通用参数
        - booster
            - eta
            - min_child_weight：最小叶子节点样本权重和
            - max_depth：树的最大深度
            - max_leaf_nodes: 树的最大的叶子数量
            - gamma 
            - max_delta_step:每棵树权重改变的最大步长
        - silent 
        - nthread 
    - 通常使用网格搜索或贝叶斯优化
        - 固定学习率**learning rate**=0.1，初始化其他参数
            - max_depth = 5 
            - min_child_weight = 1 
            - gamma = 0 
            - subsample.cosample_butree = 0.8 
        - max_depth 和 min_child_weight 参数调优, 大范围粗调 + 小范围精调
        - gamma 
        - 调整 subsample 和 colsample_bytree 
        - 正则化参数调优，选择 L1 或者 L2 正则化 
        - 降低学习率，得到最佳学习率数值 
    - n_estimator (树的个数) 
    - subsample: 样本子集的样本量
    - eta 一般[0.1, 0.2] 
    - Gamma: 用来防止过拟合，树的节点上进行进一步分支所需要的最小目标函数减少量 
    - 用于**剪枝**的参数
        - max_depth
        - colsample_bytree
        - 

# 深度学习 
## 梯度消失
- 现象
    - 靠近输出的层权值参数正常变化, 靠近输入的层权值参数几乎不变 
- 原因
    - 从网路的角度: 层数过深, 梯度的变化在反向传递时, 经过多次累乘,趋近于0 
    - 从激活函数的角度: 当输入值落入激活函数的饱和区, 导致导数为一个比较小的值, 多个值累乘导致反向传播的梯度趋近于零
- 解决方法
    - 按值截断, 把梯度压缩到一个范围内
    - 按模截断, 若大于阈值, 则 $threshold * \textbf{g}/ |g| $

## 梯度爆炸 
- 现象
    - 靠近输出的层参数变动剧烈, 靠近输入的层参数几乎不变 
    - 权重很快变成 NaN 
    - 损失函数变为 NaN  
- 原因
    - 权值初始化不合适, 初始权值使得损失函数发生剧烈波动 
- 解决方法
    - 梯度截断 
## 激活函数 
- Sigmoid  
- Tanh 
- ReLU 
- Leaky ReLU 


## DL解决过拟合 
- 从数据的角度
    - 添加噪声
    - 数据增强 
- 从模型的角度
    - early stopping : 在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练
    - L1 / L2 正则化 
    - Dropout : 
        - 实现: 让 <font color="red"> 神经元的输出以一定概率变为0 </font>  
        - 原理: 
            - 相当于对多个神经网络取平均 
            - 减少神经元之间的共适应关系 
    - BatchNormalization  
        - 解决 **Interval covariate shift** 问题, 

## 优化算法 
- MBGD: 每次利用一个 mini-batch 数据更新参数 
- Momentum: 参考动量的概念, 将前几次的梯度也加入到当前计算中, 
- adagrad: 在训练过程中自动变更学习速率, 
- adam: 利用梯度的 一阶矩估计 和 二阶矩估计, 动态的调整每个参数的learning rate 


## CNN 与 MLP 
-  


## RNN 与 LSTM 的对比 
-  

## Transformer 的结构和组件 


## Transformer 的优势, 相比于RNN 解决了什么问题 

## 评价指标  
- 分类指标
    - 混淆矩阵
        - precision: TP / TP + FP
        - recall: TP / TP + FN 

   
    - PR 
        - 横坐标 Recall 
        - 纵坐标 Precision 
        - 曲线拐点约靠近右上角, 性能越好 
        - **PR曲线在数据不平衡条件下, 不稳定, 故采用ROC** 
        - 本质是, 不同阈值下, 找出每一对(查准率, 查全率) 
        - 
     - ROC 
        - 纵轴TPR, 横轴 FPR 
        - 

    - AUC  
        - ROC曲线下的面积 
        - TPR (recall = TP / (TP + FN)) 
        - FPR (FP / FP + TN)   
        - 如果ROC面积越大，说明曲线越往左上角靠过去。那么对于任意截断点，(FPR，TPR)坐标点越往左上角（0,1）靠，说明FPR较小趋于0（根据定义得知，就是在所有真实负样本中，基本没有预测为正的样本），TRP较大趋于1（根据定义得知，也就是在所有真实正样本中，基本全都是预测为正的样本）。并且上述是对于任意截断点来说的，很明显，那就是分类器对正样本的打分基本要大于负样本的打分（一般预测值也叫打分），衡量的就是**排序能力** 
        - 由于AUC衡量的是一种排序能力
            - 对预测概率从高到低排序
            - 每个概率值设一个rank 
            - 对所有样本的预测值排序, 编号为 rk_i, 该排序代表了该概率超过的样本数量 
            - (sum( rk_{i} - M(M+1)/2)) / (M * N)  

    - F1 score  
        - 由于精确率和召回率不可能都高, 如何平衡二者, 让二者的差异不能过大
        - 使用 F1 = 2 * (P * R) / ( P + R), 即 上乘下加, **加权调和平均数**的倒数     
```python 
def AUC(label, pre):
　　"""
　　适用于ｐｙｔｈｏｎ3.0以上版本
   """
　　#计算正样本和负样本的索引，以便索引出之后的概率值
    pos = [i for i in range(len(label)) if label[i] == 1]
    neg = [i for i in range(len(label)) if label[i] == 0]
 
    auc = 0
    for i in pos:
        for j in neg:
            if pre[i] > pre[j]:
                auc += 1
            elif pre[i] == pre[j]:
                auc += 0.5
 
    return auc / (len(pos)*len(neg))
 
if __name__ == '__main__':
    label = [1,0,0,0,1,0,1,0]
    pre = [0.9, 0.8, 0.3, 0.1, 0.4, 0.9, 0.66, 0.7]
    print(AUC(label, pre))
 
    from sklearn.metrics import roc_curve, auc
    fpr, tpr, th = roc_curve(label, pre , pos_label=1)
    print('sklearn', auc(fpr, tpr))
``` 

  

- 回归指标 
    - MAE
        - 预测值与真实值之间的残差的绝对值的均值
        - L1范数误差
    - MSE 
        - 预测值与真实值之间的残差的平方和的均值
        - 如果存在离群点, 计算结果偏大, 对离群点较为敏感  
        - L2范数误差
    
    - RMSE 
        - 预测值与真实值的残差的标准差 
    - R2
        - 当前模型与基准模型的对比
        - 基准模型: y标均值与y标的误差平方和
        - 1 - (MSE(当前模型)) / (MSE(基准模型)) 
    - 


- 风控指标  
    - WOE 
        - 两种理解: 
            - 每个分箱中 **坏样本分布** 与 **好样本分布** 之间的差异 (每个bin中 坏/好)
            - **每个分箱的坏好比** 与 **总体坏好比** 的差异 (每个bin / 整体) 
            - 
    - IV 
        - 衡量好样本与坏样本分布的差异 
        - IV值越大, 说明区分能力越大, 预测能力越强 

    - KS 
        - 横轴阈值, 纵轴TPR/FPR 
        - 
    - PSI 
        -  
    - CSI 
        - 
    - AUC
        -
    - Lift 
        -    
## 风控指标相关
- Viantage, 滚动率, 迁移率 
    - Viantage 账龄分析: 分析账户成熟期和变化规律 
    - 滚动率分析: 用以定义账户好坏程度 
    - 迁移率分析: 分析不同状态之间的转化率 
- MOB 账龄
    - 定义: 资产放款月份
    - MOB0: 放款日至当月底 
    - MOB1: 放款后第二个完整月
    - MOB2: 放款后第三个完整月  
    - MOB最大值取决于**产品周期** 
- DPD 逾期天数
    - 定义: 实际还款日 - 应还款日  
- M 逾期期数
    - 逾期天数 / 设定的区间间隔 
    - M0 : 逾期期数0, 也可用C表示 
    - M1 : 逾期 1-30 天 
    - M2 : 逾期 31-60 天  
 ## 噪声处理 
    - 数据清洗  
        - 缺失值
            - >30%以上尽量不使用 
            - <10%可用中位数和RF填充 
            - 10-30%可单独看作一个类或者分箱  
        - 查看各变量与目标变量之间的相关性
    - 异常值检测 
        - IF, LOF 
        - 
    - 

## 常见数据预处理方式 
- 聚集
- 抽样 
- 降维
- 特征子集选择 
- 特征创建
- 离散化和二元化
- 变量变换 

- 变量筛选 
    - 唯一变量直接剔除
    - 方差很小小于阈值的变量直接剔除  
- 缺失值 
    - 处理方法 
        - 直接使用
        - 直接删除
        - 补全 
            - 均值插补 
            - 同类均值插补 
            - 建模预测 
            - 高维映射 
            - 多重插补 
            - 压缩感知及矩阵补全  
- 异常值 
    - 
## 特征筛选
- 过滤法
    - 离散型:  
    - Filter(过滤法)：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选 
    - Person相关系数, 互信息和最大信息系数
- 封装法  
    - Wrapper(包装法)：对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集 
    - 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后通过学习器返回的 coef_ 或者feature_importances_ 消除若干权重较低的特征, sklearn的RFE 
    - 
- 嵌入法 


- 降维
    - PCA 
        - 原理: 通过找到低维空间中数据方差最大的方向, 使得投影后的数据最大程度上保留原始信息  
        - 均值和协方差, **数据中心化处理**保证特征均值为0 
        - 求协方差矩阵, 衡量特征之间的相关性
        - 计算特征值和特征向量
        - 
## 图算法  
- leidian: 目前最新的社区划分算法, 基于
- louvain: 基于模块度 
- LPA 
- Infomap 

# 异常检测  
- 孤立森林 LF 
- LOF 局部因子检测 
- 



# 客户画像 
## 客户标签  
- 静态标签
    - 以家庭为单元:
        - 配偶特征: 国籍, 工作单位类型, **月薪收入** 等
        - 家庭特征: 总人数, 供养人口, 可支配年收入, **房产性质**,  **房产套数**, **住宅面积**, **是否有车**, **月收入**,  **月平均支出**, **月债务支出**, **总负债**, **贷款总笔数**
        - 
- 偏好标签
    - 各渠道上周交易量 
    - 小额取现上周交易量 
    - 持有产品上周余额总量 
    - 各产品上周持有金额总量 
    - 计算衰减 
    - 
## 客户活动指标活跃度统计 
- 对手活跃度 
- 卡活跃度 
- 客户活跃度 
- 渠道活跃度 
- 取现活跃度 
- 资金出入活跃度 
- POS交易商户活跃度  

## 客户收支情况统计  
- 存款收支统计 
- 贷款收支统计 
- 第三方京东收支统计  

## 客户资产指标统计 
- 产品持有明细
- 当前持有余额
- 资产汇总信息 





