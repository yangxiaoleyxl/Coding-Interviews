## 0908
### 算法题复习 
1. 数组中第 K 大的元素 - 堆排序 
```python 
# 采用堆排序, 而非快速排序的解法   

    
```  

2. 买卖股票的最佳时机 III - 三维数组DP  
```python  
if prices == []: return 0 

n = len(prices)  
# dp 表示  第 i 天交易结束时, 所能获得的最大利润, dp[天数][是否持股][卖出次数]
dp = [ [[0,0,0], [0,0,0]] for i in range(n) ]  

dp[0][0][0] = 0  
# 第1天若买入,卖出0, 最大收益为 ‘当天买入价的相反数’ 
dp[0][1][0] = -prices[0]  

# 第1天不买入, 卖出 1 或 2次, 不可能, 故最大收益为 -inf 
dp[0][0][1] = float('-inf') 
dp[0][0][2] = float('-inf')  

# 第1天买入, 但不允许卖出
dp[0][1][1] = float('-inf') 
dp[0][1][2] = float('-inf')

for i in range(1, n):
    dp[i][0][0] = 0 
    dp[i][0][1] = max(dp[i-1][1][0]+prices[i], dp[i-1][0][1])
    dp[i][0][2] = max(dp[i-1][1][1]+prices[i], dp[i-1][0][2]) 

    dp[i][1][0] = max(dp[i-1][0][0]-prices[i], dp[i-1][1][0])
    dp[i][1][1] = max(dp[i-1][0][1]-prices[i], dp[i-1][1][1])
    dp[i][1][2] = float('-inf') 

return max(dp[n-1][0][1], dp[n-1][0][2], 0)

```  

### 算法题新增  
1. 最小路径和   
```python 
class Solution:
    def minPathSum(self, grid: List[List[int]]) -> int:
        if not grid:
            return None 

        r = len(grid) 
        c = len(grid[0]) 
        dp = [[0]*c for _ in range(r)] 
        dp[0][0] = grid[0][0] 

        for i in range(r):
            for j in range(c):
                if not i and not j: 
                    continue 
                if i == 0 and j != 0:
                    dp[i][j] = dp[i][j-1] + grid[i][j] 
                if i != 0 and j == 0:
                    dp[i][j] = dp[i-1][j] + grid[i][j] 
                if i != 0 and j != 0:
                    dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j] 
        return dp[-1][-1]

```  

2. 合并两个有序链表 
```python 
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next
class Solution:
    def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -> Optional[ListNode]: 
        p1 = list1 
        p2 = list2  
        
        dummy = ListNode(0)
        p = dummy 
        while p1 and p2:
            if p1.val <= p2.val:
                p.next = p1 
                p1 = p1.next 
            else:
                p.next = p2 
                p2 = p2.next 
            p = p.next  
        p.next = p1 if p1 else p2 
        return dummy.next
```  
- 常见的双指针技巧, 在链表中用的尤其多

### 风控/ML八股文复习   
- 信贷场景 
    - 

### 风控/ML八股文新增 

#### 三种集成学习框架
- bagging  
    - 更关注**降低方差**，通过分治的策略，多个模型融合，从而减小方差 
    - 对训练集进行子抽样，对子训练集构建基模型 
- boosting   
    - 更关注**降低偏差**，聚焦于分类错误的样本，减小偏差 
    - 串行地训练基学习器 
    - 给予之前分类错误的样本更高的权重，用于训练当前的基学习器 
    - 所有基学习器加权求和
- stacking
    - 组合分类器方法  

- RF 
    - bagging算法的代表，以 CART 树作为弱分类器  
    - 通过多棵决策树的组合来降低单棵决策树的片面性 
    - 过程
        - 通过 bootstrap 得到 N 个训练子集 
        - 产生 N 棵决策树 
        - 对于分类，输出多数投票的结果
        - 对于回归，输出多棵决策树预测值的平均值  
    - 随机性体现在
        - **样本选取**的随机性
        - **特征选取**的随机性  
    - 优点
        - 高度并行化，训练速度快
        - 两个随机性，使得RF不容易过拟合 
        - 连续型和离散型变量均可处理
    - 缺点
        - 
- GBDT 
    - GBDT的第k树，学习的是之前每棵树的输出与真实值的差异（即残差），训练第 k 棵树时（计算残差时），**增大对之前分类错误样本的权重** 
    - GBDT中全是回归树，不用分类树，是因为**分类的结果相减**没有意义 
- GBDT 与 RF 的异同 
    - 都由多棵树构成，最终结果由多棵树决定 
    - RF并行，GBDT串行 
    - RF关注于减小方差，GBDT关注于减小偏差 
    - RF对异常值不敏感，GBDT对异常值非常敏感 
    - RF是所有结果多数投票或者简单平均， GBDT是加权求和  

  

- xgb特征重要性输出 
    - get_score 用于输出特征重要性 
    - import 


- LGB 和 XGB 的在原理和性能上的差异 
        - 内存和速度上的优化：
            - xgb采用的是预排序，空间消耗大。**既保留特这个，也保留特征排序的结果（例如排序后的索引）**。需要消耗训练数据2倍的内存 
            - lgb使用的是直方图算法，基本思想是：将 **连续的浮点特征值离散化成 k 个整数**， 构造一棵宽度为 k 的直方图。 遍历数据的时候，根据**离散化以后的值作为索引**，在直方图中累积统计量
        - 准确率上的优化
            - xgb 通过 **level-wise** 策略sh,  
            - lgb 通过 **leaf-wise**, 

## 特征工程 - I
- 常规步骤
    - 数据获取，数据可用性评估（覆盖率，准确率，获取难度） 
    - EDA: 对数据和特征有个大致的了解，同时进行质量检验，包括 **缺失值，异常值，重复值，一致性，正确性** 
    - 特征处理：**数据预处理（缺失值，异常值，错误值，数据格式）** 和 **特征转换（连续型，离散型，时间序列特征）** 
    - 特征构建：寻找与目标变量相关，且区分度较好的特征。常用方法**四则运算，基于业务理解的构造** 
    - 特征筛选：也可叫做 特征降维 **过滤, 包装（封装）, 嵌入法**

- 原始数据通常存在的问题
    - 真实性
    - 缺失值比例过高：判断缺失的原因，看缺失是否有风险，例如，芝麻分缺失相对而言风险较高，那么'芝麻分特征缺失'可当作一个类别来处理
    - 如何使用高基数类别型特征: 对于 **位置信息，设备信息** 这种高基数类别型变量，不能直接one-hot。**降基处理** 或 **xgb对类别数做重要性排序，重要性较高的类别做 one-hot** 
- 评分卡或集成模型中，如何衡量特征的有用性 
    - 在业务上具有良好的解释性 
    - 特征与目标变量的相关性, 衡量指标为 IV  
    - 稳定性, 特征的覆盖率, 分布, 区分效果, 
    - 覆盖率, 通常大于 70%
    - 及时性